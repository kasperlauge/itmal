{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supergruppe diskussion\n",
    "\n",
    "## § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel (eksklusiv\"Create the Workspace\" og \"Download the Data\"), og forbered mundtlig præsentation.\n",
    "\n",
    "Lav et kort resume af de enkelte underafsnit, ca. 5 til 20 liners tekst.\n",
    "\n",
    "Husk at relater til \"The Map\":\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L07/Figs/ml_supervised_map.png\" style=\"width:400px\">\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* Look at the Big Picture\n",
    "* Get the Data (eksklusiv Create the Workspace og Download the Data),\n",
    "* Discover and Visualize the Data to Gain Insights,\n",
    "* Prepare the Data for Machine Learning Algorithms,\n",
    "* Select and Train a Model,\n",
    "* Fine-Tune Your Model,\n",
    "* Launch, Monitor, and Maintain Your System,\n",
    "* Try It Out!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume: Look at the Big Picture\n",
    "\n",
    "Dette afsnit handler om at definere selve problemet. Først fremmest handler det om at finde ud af hvad formålet med den opgave man er blevet stillet er; hvad er slutmålet? Derefter skal man definere hvilken type problemstilling der er tale om. Dette inkluderer om det er supervised, unsupervised eller reinforcement learning og om det er klassifikation eller regression. I det konkrete eksempel er det et supervised regressions problem fordi der er labels, og vi forsøger at forudsige en continuert variabel. Derefter besluttes hvilket cost-funktion der skal anvendes. Der diskuteres forskellige normtyper som kan anvendes som cost funktioner såsom MAE og RMSE. I det konkrete eksempel vælges RMSE da dette virker godt på normaltfordelte dataset på grund af en begrænset mængde outliers. Til sidst handler det om at få verificeret alle de antagelser man har lavet, så man ikke ender med at bruge to måneder på en forkert opgave.\n",
    "\n",
    "#### Resume: Get the Data (eksklusiv Create the Workspace og Download the Data)\n",
    "\n",
    "I dette afsnit kigger vi på den data vi skal arbejde på. Vi kigger på hvilke features vi har til rådighed og hvilke enheder disse er målt i. Dernæste kigger vi på om vi mangler featuers for nogle datapunkter, og om der er lavet en processering af data. I det konkrete tilfælde mangler der bl.a. total_bedrooms feature data for nogle datapunkter og at median_income featuren er blevet skaleret og begrænset til 15. Vi kigger også på histogrammer for alle features for at få et overblik over datasættet. Efter dette skal der laves et træningssæt og et testsæt af datasættet. Dette skal gøres hurtigt i forløbet inden at man ved et uheld kommer til at introducere bias i ens model. Dette kan gøres ved at trække 20% tilfældige datapunkter fra det samlede datasæt. Det er vigtigt at skabe consistency i en test/trænings-split hvilket der gives nogle eksempler på hvordan man kan gøre. Fx kan man persistere trænings/test-sættene eller man kan bruge et seed i en tilfældighedsalgoritmer. Det bedste er dog ved at have noget unikt identificerbart ved ens data og bruge disse id'er til at udvælge testsættet med. Dette kan nemlig nemt replikeres selvom data hentes ned på ny. Dernæst diskuteres statrificeret udvælgelse hvilket går ud på at man skal sørge for at en testsæt er repræsentativt for ens datasæt.\n",
    "\n",
    "#### Resume: Discover and Visualize the Data to Gain Insights,\n",
    "\n",
    "I dette afsnit udforskes datasættet. Dette går i højgrad ud på at plotte scatter plots og sammenligner korrelationer mellem forskellige features. I det konkrete exempel giver det fx god mening at lave et scatter plot baseret på lokations data fra california housing datasættet. Pointen er at menneskers hjerne i høj grad er god til at se mønstre i data, så det er en god idé at visualisere det for at få et overblik over datasættet. I scatter plottet leges der med alpha parameteren der gør at det er nemmere at se koncentrationer af høje huspriser. Når vi kigger efter korrelation er det vigtigt at vide hvilke begrænsinger der er ved korrelationsanalyse. Dette illustreres i kapitlet ved at vise at korrelationer kun finder linærere sammenhænge og derfor godt kan give et forkert billede af sammenhængen mellem features. Biblioteket Pandas bruges til at udregne korrelationer og lave histogrammer ved funktionen scatter_matrix. Ved at dykke ned i de forskellige korrelationsplots kan forskellige mønstre ses. Bl.a. er det tydeligt at se at median_house_value er begrænset til 500000. Det understreges at det er vigtigt at kigge på den logiske mening mellem features og udfra det kigge på korrelationen mellem features det giver mening at kigge på.\n",
    "\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "I dette afsnit handler det om at præproccessere datasættet. Først og fremmest bliver der forklaret vigtigheden i at skrive funktioner, klasser og moduler for at kunne reproducere præprocceseringstrin. Derefter handler det om at rense datasættet for mangelende datapunkter. Vi fandt tidligere ud af at der manglede datapunkter for total_bedrooms featuren. Denne skal håndteres ved enten at smide featuren helt væk, smide datapunkterne hvor dette er sket væk eller ved at indsætte en erstatningsværdi som giver mening (fx middelværdi). Dette kan gøres vha. sklearn bibliotekets transformer kaldt en Imputer. Dernæst forklares hvordan kategoriske variable håndteres. Disse kodes først fra streng-værdier til talværdier og derefter one-hot-encodes de for at sikre at afstanden mellem disse features ikke afhænger af den ulogiske heltals værdi fra første step. Dernæst beskrives hvordan man kan skrive sine egne custom transformers for på denne måde at kunne interagere med transformation pipelines. Custom transformers kan pakke et transformationstrin ind i en logisk pakke der senere kan bruges med sklearn metodikken. Der beskrives at machine learning algoritmer håndterer uskaleret data rigtig dårligt og derfor bliver man nødt til at skalere sin data ved enten at normalisere eller standardisere. Til sidst beskrives hvordan man laver en transformations pipeline hvilket er en logisk måde at udføre præproccessering af data i iterative trin. En pipeline har et almindeligt transformations API med fit/transform og hvert step i pipelinen har det samme. Man fører så hvert transformations trin in i pipelinen via en python liste.\n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "\n",
    "Dette afsnit handler om at at bruge det præprocceserede data i en model. Da det er et regressionsproblem anvendes regressionsmodeller. Først prøves en lineær regresionsmodel som ser ud til at underfitte data'en rigtig meget. Dette kan skyldes at modellen ikke kan beskrive den ulinearitet som der højst sandsynligt er i data'en.  Dernæst vælges en mere kompleks model som er bedre til at beskrive ulinearitet. Der vælges en DecisionTreeRegressor. Denne model ser dog ud til at overfitte helt vildt idet den får en fejl på 0. For at verificere mistanken kan man køre cross validation på vha. modellen på sin data. Dette deler træningssættet op et antal lige store sæt og træner på alle dele undtagen en som fungerer som valideringssæt. Hvilken del der er valideringssæt varieres indtil alle dele har været valideringssæt. Dette giver nogle meget brugbare statistiske variable man kan bruge til at se om mistanken om overfitting er rigtig. I det konkrete tilfælde viser dette sig at være sandt. På baggrund af dette vælges en ny model; RandomForestRegressor som generelt set overfitter mindre end DecisionTreeRegressor. Der lægges vægt på at man skal prøve flere modeltyper af og gemme sine resultater til nærmere analyse af modellerne.\n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "\n",
    "Dette afsnit handler om at fintune hyperparametre. Den første metode der præsenteres til dette er GridSearchCV som vil prøve all de kombinationer af parametre af som den er blevet givet, vha. cross validation. Dette betyder at det kan være en langsommelig proces, men på denne måde har man også testet alle de mulige kombinationer. Resultatet af dette er de hyper parametre som giver det bedste output for modellen på træningssættet. Hvis man har en langsom træningsproces eller parameter spacet er stort er det dog at foretrække at bruge RandomGridSearchCV som arbejder med tilfældige variable. Her kan man fintune hvor mange gange modellen skal trænes hvilket giver mere kontrol over computerens energiforbrug. Man kan derefter kigge på den bedste model og dens performance. Ved en RandomForestRegressor kan man bl.a. få en liste ud omkring forskellige features vigtighed. Til sidst går det ud på at validere den model man har fundet på testsættet. Her vil man se hvad \"den rigtige\" performance er. Den vil typisk være en smule lavere, da der er stor sandsynlighed for at en model er fittet træningssættet. Man skal modstå fristelsen til at fintune modellen på baggrund af testsættet.\n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "Dette afsnit handler om det at føre sin model ud i produktion. Afsnittet handler primært om at opridse forskellige vigtige pointer i at have en model i produktion. Første pointe er at man skal overvåge sin model og være sikker på at opfange hvis ydeevnen falder markant. Modeller holder typisk ikke for evigt hvis ikke de bliver gentrænet med nyt data. Dette kræver typisk en menneskelig del i sin pipeline til at evaluere performance af modellen. Man skal også jævnligt evaluere de signaler som modellen modtager og kvaliteten af disse. Hvis et signal pludselig falder meget i kvalitet kan det have en meget negativ effekt på modellen. Man skal som udgangspunkt prøve at automatiseregentræningen af sin model, da det ellers kun vil ske sjældent.\n",
    "\n",
    "#### Resume: Try It Out!.\n",
    "\n",
    "Dette afsnit er primært en kort konklussion på kapitlet. Der lægges vægt på at meget arbejde ligger i præprocessering af data og overvågning af systemet og knap så meget i selve machine learning algoritmen, som dog stadig er vigtig. Det pointeres at det er vigtigt at blive tryg ved hele machine learning processen, fremfor at fokusere for meget på avancerede machine learning algoritmer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
